{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dandi Archive metadata for Qdrant vector embeddings\n",
    "\n",
    "- Customize the Dandiset blacklist as needed\n",
    "- Run the script to filter and process Dandiset metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A newer version (0.58.0) of dandi/dandi-cli is available. You are using 0.56.2\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "Could not import llama-cpp-python library. Please install the llama-cpp-python library to use this embedding model: pip install llama-cpp-python",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/dandi-search-response-ui/dandivenv/lib/python3.11/site-packages/langchain/embeddings/llamacpp.py:87\u001b[0m, in \u001b[0;36mLlamaCppEmbeddings.validate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mllama_cpp\u001b[39;00m \u001b[39mimport\u001b[39;00m Llama\n\u001b[1;32m     89\u001b[0m     values[\u001b[39m\"\u001b[39m\u001b[39mclient\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m Llama(model_path, embedding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_params)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_cpp'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/jai/dandi-search-response-ui/scripts/get_qdrant_points.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jai/dandi-search-response-ui/scripts/get_qdrant_points.ipynb#W1sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m dandi_client \u001b[39m=\u001b[39m DandiClient()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jai/dandi-search-response-ui/scripts/get_qdrant_points.ipynb#W1sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m openai_client \u001b[39m=\u001b[39m OpenaiClient()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jai/dandi-search-response-ui/scripts/get_qdrant_points.ipynb#W1sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m llama_client \u001b[39m=\u001b[39m Llama2Client()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jai/dandi-search-response-ui/scripts/get_qdrant_points.ipynb#W1sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m all_metadata \u001b[39m=\u001b[39m dandi_client\u001b[39m.\u001b[39mget_all_dandisets_metadata()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jai/dandi-search-response-ui/scripts/get_qdrant_points.ipynb#W1sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m all_metadata_formatted: \u001b[39mlist\u001b[39m[\u001b[39mdict\u001b[39m] \u001b[39m=\u001b[39m dandi_client\u001b[39m.\u001b[39mcollect_relevant_metadata(metadata_list\u001b[39m=\u001b[39mall_metadata)\n",
      "File \u001b[0;32m~/dandi-search-response-ui/scripts/../rest/clients/llama2.py:15\u001b[0m, in \u001b[0;36mLlama2Client.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdandi_client \u001b[39m=\u001b[39m DandiClient()\n\u001b[1;32m     14\u001b[0m model_path \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(Path\u001b[39m.\u001b[39mcwd()\u001b[39m.\u001b[39mparent \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbin\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmodel.bin\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings_client \u001b[39m=\u001b[39m LlamaCppEmbeddings(model_path\u001b[39m=\u001b[39;49mmodel_path)\n",
      "File \u001b[0;32m~/dandi-search-response-ui/dandivenv/lib/python3.11/site-packages/pydantic/main.py:339\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/dandi-search-response-ui/dandivenv/lib/python3.11/site-packages/pydantic/main.py:1102\u001b[0m, in \u001b[0;36mpydantic.main.validate_model\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/dandi-search-response-ui/dandivenv/lib/python3.11/site-packages/langchain/embeddings/llamacpp.py:91\u001b[0m, in \u001b[0;36mLlamaCppEmbeddings.validate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m     89\u001b[0m     values[\u001b[39m\"\u001b[39m\u001b[39mclient\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m Llama(model_path, embedding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_params)\n\u001b[1;32m     90\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m---> 91\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mModuleNotFoundError\u001b[39;00m(\n\u001b[1;32m     92\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCould not import llama-cpp-python library. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease install the llama-cpp-python library to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39muse this embedding model: pip install llama-cpp-python\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m     )\n\u001b[1;32m     96\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     97\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     98\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not load Llama model from path: \u001b[39m\u001b[39m{\u001b[39;00mmodel_path\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReceived error \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m     )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: Could not import llama-cpp-python library. Please install the llama-cpp-python library to use this embedding model: pip install llama-cpp-python"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "\n",
    "dandiset_blacklist = [\n",
    "    \"000545\",\n",
    "    \"000470\",\n",
    "    \"000411\",\n",
    "    \"000529\",\n",
    "    \"000299\",\n",
    "    \"000029\",\n",
    "    \"000027\",\n",
    "    \"000126\",\n",
    "    \"000544\",\n",
    "    \"000068\",\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "notebook_path = os.path.abspath('.')\n",
    "sys.path.append(os.path.join(notebook_path, '..'))\n",
    "\n",
    "from rest.clients.dandi import DandiClient\n",
    "from rest.clients.openai import OpenaiClient\n",
    "from rest.clients.llama2 import Llama2Client\n",
    "\n",
    "dandi_client = DandiClient()\n",
    "openai_client = OpenaiClient()\n",
    "llama_client = Llama2Client()\n",
    "\n",
    "all_metadata = dandi_client.get_all_dandisets_metadata()\n",
    "all_metadata_formatted: list[dict] = dandi_client.collect_relevant_metadata(metadata_list=all_metadata)\n",
    "print(\"START: Number of items:\", len(all_metadata_formatted))\n",
    "\n",
    "filtered_all_metadata_formatted = []\n",
    "for i, dandiset in enumerate(all_metadata_formatted):\n",
    "    if not any(item in str(dandiset[\"dandiset_id\"]).split(\":\")[-1] for item in dandiset_blacklist):\n",
    "        filtered_all_metadata_formatted.append(dandiset)\n",
    "    else:\n",
    "        print(f\"REMOVED -- {dandiset['dandiset_id']}: {dandiset['title']}\")\n",
    "\n",
    "num_not_removed = len(dandiset_blacklist) - (len(all_metadata_formatted) - len(filtered_all_metadata_formatted))\n",
    "if num_not_removed:\n",
    "    print(f\"NOTE: {num_not_removed} blacklisted dandiset(s) not removed.\")\n",
    "\n",
    "print(\"END: Number of items:\", len(filtered_all_metadata_formatted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overwrite current Qdrant points in `data/qdrant_points.json`\n",
    "\n",
    "- Make sure to run the previous script to filter Dandiset metadata before running this script\n",
    "- Choose a valid embedding model for which the qdrant points should be generated/updated (or leave model field empty to update all model qdrant points)\n",
    "- Run the script to retrieve embeddings and save them to `data/qdrant_points.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of emb items: 164\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "\n",
    "# choose one of: \"ada002\" or \"llama2\" or leave empty to update all model qdrant points\n",
    "model = \"\"\n",
    "\n",
    "# -----------------------------\n",
    "\n",
    "if model == \"ada002\":\n",
    "    emb = openai_client.get_embeddings(\n",
    "        metadata_list=filtered_all_metadata_formatted,\n",
    "        save_to_file=True\n",
    "    )\n",
    "    print(\"Number of emb items:\", len(emb))\n",
    "elif model == \"llama2\":\n",
    "    emb = llama_client.get_embeddings(\n",
    "        metadata_list=filtered_all_metadata_formatted,\n",
    "        save_to_file=True\n",
    "    )\n",
    "    print(\"Number of emb items:\", len(emb))\n",
    "elif not model:\n",
    "    emb1 = openai_client.get_embeddings(\n",
    "        metadata_list=filtered_all_metadata_formatted,\n",
    "        save_to_file=True\n",
    "    )\n",
    "    emb2 = llama_client.get_embeddings(\n",
    "        metadata_list=filtered_all_metadata_formatted,\n",
    "        save_to_file=True\n",
    "    )\n",
    "    print(\"Number of openai emb items:\", len(emb1))\n",
    "    print(\"Number of llama2 emb items:\", len(emb2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dandivenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
